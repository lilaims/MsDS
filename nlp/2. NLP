
Word reprn
N-gram 
Word2Vec
Bag of n words

Sequence Classification - 
Part of Speech Tagging
Morphological Tagging
HMM
MEMM
Conditional Random Fields

information Extraction - 
Named Entity Recognition , Relation Extraction , Temporal Expression Extraction
Temporal Tagger , Temporal Normalization , 
Entity Linking

Lemmatization 

Syntax


Modalities

Algorithms
N-gram
Hidden Markov Model - HMM
Maximum Entropy Markov Model - MEMM



---------------------
Pretrained models, like word2vec, make it easy to get started but may lack
 domain-specific words needed for a high-accuracy text analytics application.

Modalitites - visual, auditory, kinaesthetic , olfactory and gustatory


Morphological tagging is the task of assigning labels to a sequence of tokens that describe them morphologically.
As compared to Part-of-speech tagging, morphological tagging also considers morphological features, such as case, gender or the tense of verbs.

N-gram
coupling n words 
Probabilities assigned to words based on of previous word
Predicting the next word

Assigning probability 
 - based on frequency of words used
 - Markov assumption 
 - chain rule of probability
 - an intuitive way of assigning probabilities - Maximum liklihood function
The assumption that the probability of a word depends only on the previous word is
Markov called a Markov assumption. Markov models are the class of probabilistic models
that assume we can predict the probability of some future unit without looking too
far into the past. We can generalize the bigram (which looks one word into the past)
n-gram to the trigram (which looks two words into the past) and thus to the n-gram (which
looks nâˆ’1 words into the past).

**
Markov Chain
Set of n states , transition probability matrix , start and end state ,
observation , observation and liklihood
