Transformer
Bert
Masked Language Modelling

Attention
Encoder Decoder 

Neural Network

Prompt Design
Prompt Engineering
----------------------
Input -> Input Embedding -> Encoder -> Decoder -> Output Embedding -> Output

Encoder -> Attention -> Feed Forward Neural Network

Attention - [Query,Key,Value] Vectors

[X*Wq]=Q,[X*Wk]=K,[X*Wv]=V
Q- [3*3] , Key - [3*3] , Value- [3*3]
Z = Softmax[(Q*K_T)/sqrt(dk)] V

Concatenate Z matrices

bert used by google , was trained on wikipedia corpus

Usage - Classification (Single , Pair) , Question Answering , Sentence Tagging
Bert 1 2 3
1. Masked Language Modelling - masking words in a sentence for efiicient training 
   a lot of masking makes removes context
2. Next Sentence Prediction(NPS) - 

Bert Input Embeddings - Token Embeddings , Segment Embeddings , Position Embeddings
