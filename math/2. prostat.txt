Variable vs Random Variable
Probability Distribution
PDF 
CDF
Other parameters of scale

Law of Large Numbers
Monte Carlo Integration
Central Limit Theorem 
Itertive Logarithms

Estimators
--------------------------------------------
Random Varible - it has whole set of values and it can take any of those randomly.
variable can have only one value at a time.

a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment.It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).

A pf gives a probability, so it cannot be greater than one. A pdf f(x), however, may give a value greater than one for some values of x, since it is not the value of f(x) but the area under the curve that represents probability.

The term "likelihood" is often used in a slightly different context when discussing probability mass functions (PMFs) and probability density functions (PDFs).

In statistics:

1. **Probability Mass Function (PMF):**
   - For a discrete random variable, the PMF gives the probability that the random variable takes on a specific value.
   - The term "probability mass" is used because, for discrete variables, the probability is concentrated at specific points.
   - The likelihood of a specific outcome is given by the probability assigned to that particular value by the PMF.

   For example, in the context of rolling a fair six-sided die, the likelihood of getting a 3 is given by the probability mass assigned to the value 3 in the PMF.

2. **Probability Density Function (PDF):**
   - For a continuous random variable, the PDF gives the likelihood of the variable falling within a particular interval.
   - The term "density" is used because, for continuous variables, probability is spread over intervals rather than concentrated at specific points.
   - The likelihood of a range of values is given by the area under the curve of the PDF over that range.

   For example, in the context of a standard normal distribution, the likelihood of the variable falling within a certain range is given by the integral of the PDF over that range.

In summary, while the specific term "likelihood" is more commonly associated with probabilities in the context of likelihood functions in statistical inference, the concept of likelihood is inherent in both PMFs and PDFs. In the discrete case, the likelihood is associated with specific values, and in the continuous case, it's associated with intervals.
Discrete and Continuous Probability Distribution

In probability theory, a probability density function (PDF), density function, or density of an absolutely continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would be equal to that sample.[2][3] Probability density is the probability per unit length, in other words, while the absolute likelihood for a continuous random variable to take on any particular value is 0 (since there is an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would be close to one sample compared to the other sample.

In a more precise sense, the PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value. This probability is given by the integral of this variable's PDF over that range—that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range. The probability density function is nonnegative everywhere, and the area under the entire curve is equal to 1.

The terms probability distribution function and probability function have also sometimes been used to denote the probability density function. However, this use is not standard among probabilists and statisticians. In other sources, "probability distribution function" may be used when the probability distribution is defined as a function over general sets of values or it may refer to the cumulative distribution function, or it may be a probability mass function (PMF) rather than the density. "Density function" itself is also used for the probability mass function, leading to further confusion.[4] In general though, the PMF is used in the context of discrete random variables (random variables that take values on a countable set), while the PDF is used in the context of continuous random variables.

Suppose bacteria of a certain species typically live 4 to 6 hours. The probability that a bacterium lives exactly 5 hours is equal to zero. A lot of bacteria live for approximately 5 hours, but there is no chance that any given bacterium dies at exactly 5.00... hours. However, the probability that the bacterium dies between 5 hours and 5.01 hours is quantifiable. Suppose the answer is 0.02 (i.e., 2%). Then, the probability that the bacterium dies between 5 hours and 5.001 hours should be about 0.002, since this time interval is one-tenth as long as the previous. The probability that the bacterium dies between 5 hours and 5.0001 hours should be about 0.0002, and so on.

In this example, the ratio (probability of dying during an interval) / (duration of the interval) is approximately constant, and equal to 2 per hour (or 2 hour−1). For example, there is 0.02 probability of dying in the 0.01-hour interval between 5 and 5.01 hours, and (0.02 probability / 0.01 hours) = 2 hour−1. This quantity 2 hour−1 is called the probability density for dying at around 5 hours. Therefore, the probability that the bacterium dies at 5 hours can be written as (2 hour−1) dt. This is the probability that the bacterium dies within an infinitesimal window of time around 5 hours, where dt is the duration of this window. For example, the probability that it lives longer than 5 hours, but shorter than (5 hours + 1 nanosecond), is (2 hour−1)×(1 nanosecond) ≈ 6×10−13 (using the unit conversion 3.6×1012 nanoseconds = 1 hour).

There is a probability density function f with f(5 hours) = 2 hour−1. The integral of f over any window of time (not only infinitesimal windows but also large windows) is the probability that the bacterium dies in that window.

---------------------------
******
Unlike a probability, a probability density function can take on values greater than one

Not every probability distribution has a density function: the distributions of discrete random variables do not; nor does the Cantor distribution, even though it has no discrete component, i.e., does not assign positive probability to any individual point.

A distribution has a density function if and only if its cumulative distribution function F(x) is absolutely continuous. In this case: F is almost everywhere differentiable, and its derivative can be used as probability density:

******
-----------------------------
Law of Large Numbers - Strong & weak Law
 behavior of sample averages or means as the size of the sample increases

Strong Law
with probability 1, the sample mean converges almost surely to the true mean as the sample size increases.
the sample average almost surely equals the population mean as the sample size approaches infinity

Weak Law
as you take more samples and calculate their average, that average gets closer to the true mean of the entire population
sample mean of a sequence of independent and identically distributed random variables converges in probability to the expected value (mean) of the distribution as the sample size increases


Monte carlo Integration
Algebraic Integration - integ(x^2)dx = x^3/3|a b
Numeric Integration - limiting sum
Limiting Sum:
If you're referring to the concept of a limit of a sum, it could be related to the definition of an integral. The definite integral of a function 
�
(
�
)
f(x) over an interval 
[
�
,
�
]
[a,b] is defined as the limit of a sum:

lim
⁡
�
→
∞
∑
�
=
1
�
�
(
�
�
)
Δ
�
lim 
n→∞
​
 ∑ 
i=1
n
​
 f(x 
i
​
 )Δx

where 
Δ
�
=
�
−
�
�
Δx= 
n
b−a
​
  is the width of each subinterval and 
�
�
x 
i
​
  is a point in the 
�
i-th subinterval.

Limiting Integration:
If you're referring to limiting integration in a broader sense, it could relate to techniques like improper integrals, where the limits of integration are infinite or the function has singularities.

Numerical Integration:
In numerical analysis, there are methods like the trapezoidal rule or Simpson's rule, which approximate integration by dividing the area under a curve into smaller regions. These methods effectively involve a sum of function evaluations within each subinterval, and the limit of this sum approaches the integral as the number of subintervals approaches infinity.


Central Limit Theorem
