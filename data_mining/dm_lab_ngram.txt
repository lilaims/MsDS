unigram 
bigram
n-gram

Perplexity
Cross Entropy

Maximum Likelihood Estimation
Negative Log Likelihood Loss - also used for calculating cross entropy
Cross Entropy - also used for calculating Perplexity
Perplexity

---------------------------------------------

unigram - one word , bigram - 2 words , n-gram - n-words coupled together
Count of each word in corpus
Perplexity - The perplexity measures the amount of “randomness” in our model. 
  the perplexity is 3 (per word) then that means the model had a 1-in-3 chance of guessing (on average) 
  the next word in the text. 
quality of language model
low perplexity better at predicting model 

Cross Entropy - 


Cross Entropy = total neg log probs/length
Perplexity = 2 * Cross Entropy

__________________________________________________
from collections import Counter
from math import log

def tokenize_sentence(sentence, order):
    """Returns a list of tokens with the correct numbers of initial
    and end tags (this is meant ot be used with a non-backoff model!!!)
    
    :sentence: a string of text
    :param: order is the order of the language model
        (1 = unigram, 2 = bigram, 3 =trigram etc.)
    """
    tokens = sentence.split()
    tokens = ['<s>'] * (order-1) + tokens + ['</s>']
    return tokens
