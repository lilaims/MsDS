Classification
[Linear , Non Linear] , [Binary , MultiClass]

K Nearest Neighbhours (KNN)
Logistic Regression
Naive Bayes - Gaussian Naive Bayes
Random Forest
Support Vector machines

-Optimization
  Logistic Regression

-Loss
  Cross Entropy Loss , (negative log likelihood)NLL Loss
  maximum likelihood estimation
  negative log likelihood minimization
  BCE Loss

-Activation Functions - for the above loss, based on binary and multiclass
  Sigmoid
  Softmax

- Problem 
 Imbalanced data set - imbalance of classes in training and val datasets

[Summary , Latest , Libraries , Mathematics , Intuition , Issues]
---------------------------

Story -
Linear Classifiers can be binary or multiclass
w1*1+w2*x1+w3*x2+w4*x3...=0        balance of equation on both sides
dot product of w and x [w][x]= y predicted labels
y actual labels

calculate likelihood to see how well the parameters predict the data
calculate negative log likelihood
calculate the accuracy to check the correct predictions
calculate the error rate to see the wrong predictions

Optimization 
Optimize the model to get better performance parameters
increase likelihood , accuracy  , decrease the error rate

Logistic Regression - Exhaustive Search , Gradient Descent
Optimization strategy
Logistic "regression" classifiers use the likelihood function  L  as the objective function. 


Steps - 
Based on a model output labels are predicted binary or multiclass
based on labels log of labels is calculated , which is log predicted probabilities
using this loss is calculated (nll or cross entropy)

Activation Functions - transform the summed weighted input from the node

____________________________________________
Cross Entropy 
In the discrete setting, given two probability distributions p and q, their cross-entropy is defined as
 = p log q


_____________________________________________
Maximum Likelihood Estimation
y theta,i = sigma(f_theta(i))
sigma is the activation function
sigma(z) = 1 / 1+exp(-z)

likelihood = P(D|theta) = prod i to n (y_theta^y_i(1-y_theta)^1-y_i)
the main objective is to find \theta that maximizes the likelihood of observing the data

Negative Log Liklihood - 
sum up the correct entries that encode log probabilities

log P(D|theta) = prod i to n (y_i * Log y_theta + (1-y_i) * Log 1-y_theta)

For Binary Classification y_i is the output so the value is 0 or 1.

y_hat_i: the predicted probability of the ith data point being positive
(1-y_hat_i): the predicted probability of the ith data point being negative

Summing up the correct entries (binary case)
The following animation further illustrates this idea of picking the correct entries to sum. It consists of the following steps:

Start with predicted probabilities for the positive class (y_hat). If we were given raw prediction values, apply sigmoid to make it a probability.
Compute the probabilities for the negative class (1-y_hat).
Compute the log probabilities.
Summing up the log probabilities associated with the true labels.

Given the rewritten log-likelihood above, it is tempting to directly apply it to the multiclass (with C classes) setting,
where y now takes value from 0 up to C-1. This almost works out except that we need to make sure y_hat_i defines a probability distribution, namely, 1)
it is bounded between zero and one, and 2) the distribution sums up to 1. In the binary setting,
these two conditions were taken care of by the sigmoid activation and an implicit assumption that 
“not positive means negative.”

It turns out that the softmax function is what we are after

Starting with predicted values (not yet the probabilities) z.
Transform the values into class probabilities (y_hat) using softmax and then take the log probabilities (log y_hat).
Summing up the log probabilities associated with the true labels.

Use BCELoss and BCEWithLogitsLoss when
Both h and y are one-dimensional, and y takes either zero or one.

Use BCELoss if h is the probability of a data point being positive.
Use BCEWithLogits if h is the logits, i.e., you want to use the sigmoid function to activate your raw prediction values into a probability.
Use NLLLoss and CrossEntropyLoss when
h is two-dimensional and y is one-dimensional, taking values of zero up to C-1 with C classes.

Use NLLLoss if h encodes log-likelihood (it essentially performs the masking step followed by mean reduction).
Use CrossEntropyLoss if h encodes raw prediction values that need to be activated using the softmax function.
In high-dimensional spaces, the concept of distance loses its meaning, and the algorithm might perform poorly. This is known as the curse of dimensionality.
___________________________________________________________________

K nearest Neighbhours - A small 
k can make the algorithm sensitive to noise, while a large 
k can make the algorithm less sensitive to local patterns

KNN is sensitive to the scale of features. Therefore, it's often essential to scale the data before applying KNN.
1. The first step in KNN is to define a distance metric that measures the similarity or dissimilarity between data points
2. The KNN algorithm requires a labeled dataset, where each data point is associated with a class label (for classification) or a 
numerical value (for regression). This dataset is used to find the k-nearest neighbors for a given test sample.
3. k is the number of neighbors

Prediction for Classification:

To classify a new data point, the algorithm computes the distance between this point and all data points in the training dataset.
It then selects the k data points with the smallest distances (nearest neighbors).
These k data points "vote" for the class label of the new data point.
The class label with the most votes among the k neighbors is assigned to the new data point.
Prediction for Regression:

For regression tasks, the algorithm follows a similar process to classify a new data point.
Instead of voting, it calculates the average (or weighted average) of the target values of the k nearest neighbors.
This average is used as the prediction for the new data point.

Hyperparameter Tuning: Choosing the right distance metric and the value of k is critical for the algorithm's performance. 
This is often done through cross-validation or other hyperparameter tuning techniques.

____________________________________________________
Number of samples of class 0 in the new training dataset: 25
Number of samples of class 1 in the new training dataset: 25
Number of samples of class 2 in the new training dataset: 5


Number of samples of class 0 in the new validation dataset: 25
Number of samples of class 1 in the new validation dataset: 25
Number of samples of class 2 in the new validation dataset: 5
